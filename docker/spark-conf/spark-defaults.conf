#!/bin/bash
# streaming/setup_streaming.sh
# Setup script for Spark Streaming environment

set -e

echo "🚀 Setting up Spark Streaming Environment..."

# Colors for output
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

# Function to print colored output
print_status() {
    echo -e "${GREEN}[INFO]${NC} $1"
}

print_warning() {
    echo -e "${YELLOW}[WARNING]${NC} $1"
}

print_error() {
    echo -e "${RED}[ERROR]${NC} $1"
}

# Check if Python is installed
if ! command -v python3 &> /dev/null; then
    print_error "Python3 is not installed. Please install Python3 first."
    exit 1
fi

# Check if Java is installed (required for Spark)
if ! command -v java &> /dev/null; then
    print_warning "Java is not installed. Installing OpenJDK 11..."
    
    # Install Java based on OS
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        sudo apt-get update
        sudo apt-get install -y openjdk-11-jdk
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        brew install openjdk@11
    else
        print_error "Please install Java 11 manually for your OS"
        exit 1
    fi
fi

# Set JAVA_HOME if not set
if [ -z "$JAVA_HOME" ]; then
    if [[ "$OSTYPE" == "linux-gnu"* ]]; then
        export JAVA_HOME="/usr/lib/jvm/java-11-openjdk-amd64"
    elif [[ "$OSTYPE" == "darwin"* ]]; then
        export JAVA_HOME="/usr/local/opt/openjdk@11"
    fi
    print_status "JAVA_HOME set to: $JAVA_HOME"
fi

# Create necessary directories
print_status "Creating directory structure..."
mkdir -p streaming/data/{checkpoints,output}
mkdir -p data/processed/streaming
mkdir -p streaming/logs

# Install required Python packages
print_status "Installing Python dependencies..."
pip3 install pyspark==3.4.0
pip3 install kafka-python
pip3 install pyyaml

# Download Kafka connector JARs for Spark
SPARK_JARS_DIR="streaming/jars"
mkdir -p $SPARK_JARS_DIR

print_status "Downloading Spark-Kafka integration JAR..."
KAFKA_JAR_URL="https://repo1.maven.org/maven2/org/apache/spark/spark-sql-kafka-0-10_2.12/3.4.0/spark-sql-kafka-0-10_2.12-3.4.0.jar"
KAFKA_CLIENTS_JAR_URL="https://repo1.maven.org/maven2/org/apache/kafka/kafka-clients/3.4.0/kafka-clients-3.4.0.jar"

if [ ! -f "$SPARK_JARS_DIR/spark-sql-kafka-0-10_2.12-3.4.0.jar" ]; then
    curl -L $KAFKA_JAR_URL -o $SPARK_JARS_DIR/spark-sql-kafka-0-10_2.12-3.4.0.jar
fi

if [ ! -f "$SPARK_JARS_DIR/kafka-clients-3.4.0.jar" ]; then
    curl -L $KAFKA_CLIENTS_JAR_URL -o $SPARK_JARS_DIR/kafka-clients-3.4.0.jar
fi

# Create environment variables file
print_status "Creating environment configuration..."
cat > streaming/.env << EOF
# Spark Streaming Environment Variables
JAVA_HOME=$JAVA_HOME
SPARK_HOME=/opt/spark
PYTHONPATH=\$SPARK_HOME/python:\$SPARK_HOME/python/lib/py4j-*-src.zip:\$PYTHONPATH

# Kafka Configuration
KAFKA_SERVERS=localhost:9092
KAFKA_TOPIC=ecommerce_transactions

# Spark Configuration
SPARK_JARS_DIR=./streaming/jars
SPARK_LOG_LEVEL=WARN

# Output Configuration
OUTPUT_PATH=./streaming/data/output
CHECKPOINT_PATH=./streaming/data/checkpoints
EOF

# Create run script
print_status "Creating run script..."
cat > streaming/run_streaming.sh << 'EOF'
#!/bin/bash
# Load environment variables
source streaming/.env

# Set Spark configuration
export PYSPARK_PYTHON=python3
export PYSPARK_DRIVER_PYTHON=python3

# Run Spark streaming job with necessary JARs
spark-submit \
    --jars "${SPARK_JARS_DIR}/spark-sql-kafka-0-10_2.12-3.4.0.jar,${SPARK_JARS_DIR}/kafka-clients-3.4.0.jar" \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.4.0 \
    --conf spark.sql.adaptive.enabled=true \
    --conf spark.sql.adaptive.coalescePartitions.enabled=true \
    --conf spark.serializer=org.apache.spark.serializer.KryoSerializer \
    streaming/spark_streaming_job.py
EOF

chmod +x streaming/run_streaming.sh

# Create alternative Python run script (if spark-submit is not available)
print_status "Creating Python run script..."
cat > streaming/run_streaming_python.py << 'EOF'
#!/usr/bin/env python3
"""
Alternative runner for Spark Streaming job using Python directly
Use this if spark-submit is not available in your environment
"""

import os
import sys
from pyspark.sql import SparkSession

# Add current directory to path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def setup_spark_with_kafka():
    """Setup Spark session with Kafka integration."""
    
    # Kafka JAR files
    jars_dir = "streaming/jars"
    kafka_jar = f"{jars_dir}/spark-sql-kafka-0-10_2.12-3.4.0.jar"
    kafka_clients_jar = f"{jars_dir}/kafka-clients-3.4.0.jar"
    
    # Check if JAR files exist
    if not os.path.exists(kafka_jar) or not os.path.exists(kafka_clients_jar):
        print("❌ Kafka JAR files not found. Please run setup_streaming.sh first.")
        sys.exit(1)
    
    # Create Spark session
    spark = SparkSession.builder \
        .appName("EcommerceStreamingAnalytics") \
        .config("spark.jars", f"{kafka_jar},{kafka_clients_jar}") \
        .config("spark.sql.adaptive.enabled", "true") \
        .config("spark.sql.adaptive.coalescePartitions.enabled", "true") \
        .config("spark.serializer", "org.apache.spark.serializer.KryoSerializer") \
        .getOrCreate()
    
    return spark

if __name__ == "__main__":
    # Import and run the streaming job
    from spark_streaming_job import EcommerceStreamProcessor
    
    # Setup Spark session
    spark = setup_spark_with_kafka()
    
    # Initialize processor
    processor = EcommerceStreamProcessor()
    processor.spark = spark  # Use our configured Spark session
    
    try:
        # Run the streaming pipeline (without creating new Spark session)
        kafka_stream = processor.read_kafka_stream()
        parsed_stream = processor.parse_json_data(kafka_stream)
        aggregations = processor.create_real_time_aggregations(parsed_stream)
        queries = processor.setup_output_sinks(aggregations)
        processor.monitor_streams(queries)
        
    except KeyboardInterrupt:
        print("Application stopped by user")
    except Exception as e:
        print(f"Application failed: {e}")
    finally:
        spark.stop()
EOF

chmod +x streaming/run_streaming_python.py

# Create test script to validate setup
print_status "Creating test script..."
cat > streaming/test_streaming_setup.py << 'EOF'
#!/usr/bin/env python3
"""Test script to validate Spark Streaming setup."""

import os
import sys
from kafka import KafkaConsumer
from kafka.errors import KafkaError

def test_kafka_connection():
    """Test Kafka connection."""
    try:
        consumer = KafkaConsumer(
            'ecommerce_transactions',
            bootstrap_servers=['localhost:9092'],
            consumer_timeout_ms=5000
        )
        print("✅ Kafka connection successful")
        consumer.close()
        return True
    except Exception as e:
        print(f"❌ Kafka connection failed: {e}")
        return False

def test_spark_import():
    """Test Spark imports."""
    try:
        from pyspark.sql import SparkSession
        from pyspark.sql.functions import from_json
        print("✅ PySpark imports successful")
        return True
    except Exception as e:
        print(f"❌ PySpark imports failed: {e}")
        return False

def test_directories():
    """Test directory structure."""
    required_dirs = [
        "streaming/data/checkpoints",
        "streaming/data/output", 
        "data/processed/streaming",
        "streaming/jars"
    ]
    
    all_exist = True
    for dir_path in required_dirs:
        if os.path.exists(dir_path):
            print(f"✅ Directory exists: {dir_path}")
        else:
            print(f"❌ Directory missing: {dir_path}")
            all_exist = False
    
    return all_exist

def test_jar_files():
    """Test JAR files exist."""
    jar_files = [
        "streaming/jars/spark-sql-kafka-0-10_2.12-3.4.0.jar",
        "streaming/jars/kafka-clients-3.4.0.jar"
    ]
    
    all_exist = True
    for jar_file in jar_files:
        if os.path.exists(jar_file):
            print(f"✅ JAR file exists: {jar_file}")
        else:
            print(f"❌ JAR file missing: {jar_file}")
            all_exist = False
    
    return all_exist

if __name__ == "__main__":
    print("🧪 Testing Spark Streaming Setup...")
    print("-" * 50)
    
    tests = [
        ("Directory Structure", test_directories),
        ("JAR Files", test_jar_files),
        ("PySpark Imports", test_spark_import),
        ("Kafka Connection", test_kafka_connection)
    ]
    
    results = []
    for test_name, test_func in tests:
        print(f"\n🔍 Testing {test_name}...")
        result = test_func()
        results.append((test_name, result))
    
    print("\n" + "="*50)
    print("📊 TEST RESULTS:")
    print("="*50)
    
    all_passed = True
    for test_name, result in results:
        status = "✅ PASS" if result else "❌ FAIL"
        print(f"{test_name}: {status}")
        if not result:
            all_passed = False
    
    if all_passed:
        print("\n🎉 All tests passed! Streaming setup is ready.")
    else:
        print("\n⚠️  Some tests failed. Please check the setup.")
        sys.exit(1)
EOF

chmod +x streaming/test_streaming_setup.py

print_status "Setup completed successfully! 🎉"
print_status ""
print_status "Next steps:"
print_status "1. Run the test script: python3 streaming/test_streaming_setup.py"
print_status "2. Make sure Kafka is running: docker-compose up -d kafka"
print_status "3. Start the producer: python3 ingestion/producer.py"
print_status "4. Run streaming job: ./streaming/run_streaming.sh"
print_status "   OR: python3 streaming/run_streaming_python.py"
print_status ""
print_status "Check streaming/logs/ for application logs"
print_status "Check streaming/data/output/ for processed data"